import { Injectable } from '@nestjs/common';
import { EnvService } from 'ðŸ“šlibs/modules/env/env.service';
import { EnvEnum } from 'ðŸ“šlibs/modules/env/env.enum';
import { HttpService } from '@nestjs/axios';
import { catchError, firstValueFrom } from 'rxjs';

@Injectable()
export class OpenAiService {
  private openAIHeader: { [key in string] };
  private OPEN_AI_MODEL = 'text-ada-001';
  private OPEN_AI_MAX_TOKEN = 50;
  private OPEN_AI_TEMPERATURE = 0;

  constructor(private readonly envService: EnvService, private readonly httpService: HttpService) {
    this.openAIHeader = {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${this.envService.get(EnvEnum.OPENAI_API_KEY)}`,
    };
  }

  public async promptChatGPT(prompt: string): Promise<{ text: string }> {
    /*
     * max_tokens í”„ë¡œí¼í‹°ëŠ” GPT ëª¨ë¸ ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤. Ex) max_tokensê°€ 50ì´ë©´ 50ê°œì˜ í† í°ì„ ìƒì„± í•œ í›„ ëª¨ë¸ì˜ ì‘ë‹µì´ ìž˜ë¦¬ê²Œ ë©ë‹ˆë‹¤.
     * temperature í”„ë¡œí¼í‹°ëŠ” ì°½ì˜ì„±ì˜ ì •ë„ìž…ë‹ˆë‹¤. temperatureê°€ ë‚®ìœ¼ë©´ ê°€ìž¥ ê°€ëŠ¥ì„±ì´ ë†’ê³  ë³´ìˆ˜ì ì¸ ì‘ë‹µì„ ë±‰ì–´ë‚´ê³  1.0ê³¼ ê°™ì´ ë†’ìœ¼ë©´ ì°½ì˜ì ì¸ ë‹µë³€ì´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.
     */
    const data = {
      model: this.OPEN_AI_MODEL,
      prompt,
      max_tokens: this.OPEN_AI_MAX_TOKEN,
      temperature: this.OPEN_AI_TEMPERATURE,
    };

    const response = await firstValueFrom(
      this.httpService.post('https://api.openai.com/v1/completions', data, { headers: this.openAIHeader }).pipe(
        catchError((error) => {
          throw error;
        }),
      ),
    );
    const responseData = response.data; //.choices[0].text;
    if (typeof responseData === 'string') return { text: responseData };
    return responseData;
  }
}
